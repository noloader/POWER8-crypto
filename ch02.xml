<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch02">
  <title>Vector programming</title>
  <para>Several topics need to be discussed to minimize trouble when using the Altivec and POWER8 extensions. They include PowerPC compilers and options, Altivec headers, machine endianness, vector datatypes, memory and alignment, and loads and stores. It is enough information to get to the point you can use AES and SHA but not much more.</para>
  <para>Memory alignment, loads, stores and shifts will probably cause the most trouble for someone new to PowerPC vector programming. If you are new to the platform you may want to read this chapter twice. If you are experienced with the platform then you probably want to skip this chapter.</para>
  <para>Two compilers are used for testing. The first is GCC and the second is IBM XL C/C++ (XLC). Both produce high quality code. The GCC and XLC compilers are mostly the same but accept slightly different options. LLVM's Clang was not tested because the <link linkend="intro_cfarm">compile farm </link> lacks a Clang installation.</para>
  <section id="gcc_compiler" xreflabel="GCC Compiler">
    <title>GCC compiler</title>
    <para><indexterm><primary>GCC</primary></indexterm>Compiling a test program with GCC will generally look like below. The important part is <systemitem>-mcpu=power8</systemitem> which selects the POWER8 Instruction Set Architecture (ISA). The minimum architecture required for Altivec with GCC is <systemitem>-march=power4</systemitem>.</para>
    <programlisting><?db-font-size 75% ?>$ g++ -mcpu=power8 -maltivec test.cxx -o test.exe</programlisting>
    <para>GCC consumes <systemitem>-g</systemitem> and <systemitem>-O3</systemitem>. If you want Position Independent Code then use <systemitem>-fPIC</systemitem>.</para>
    <para>GCC uses integer math to calculate the effective memory addresses from a memory address and offset. That is, if you have an array <systemitem>uint32_t vals = {0,1,2,3,4,5,6,7}</systemitem>, and you call <systemitem>uint32x4_p v = vec_xl(vals, 4)</systemitem>, then the vector will have the value <systemitem>{1,2,3,4}</systemitem>. That is, GCC uses the address of <systemitem>vals</systemitem> and simply adds <systemitem>4</systemitem> to it. Effective addresses are discussed in <xref linkend="effective_address"/>, and loads and stores are discussed in <xref linkend="altivec_aligned_ld_st"/>.</para>
    <para>If you work on the <link linkend="intro_cfarm">Compile Farm</link> then be mindful of the default GCC compiler. It may be GCC 4.8.5 which is a bit old and unsupported. You usually enjoy better code generation and performance with a modern GCC like 7.2. A newer compiler is usually located at <systemitem>/opt/cfarm/gcc-latest</systemitem>.</para>
  </section>
  <section id="xlc_compiler" xreflabel="XLC Compiler">
    <title>XLC compiler</title>
    <para><indexterm><primary>IBM</primary></indexterm><indexterm><primary>XLC</primary></indexterm>Compiling a test program with IBM XL C/C++ will generally look like below. The important parts are the C++ compiler name of <systemitem>xlC</systemitem>, and <systemitem>-qarch=pwr8</systemitem> which selects the POWER8 ISA. The minimum architecture required for Altivec with XLC is <systemitem>-qarch=pwr6</systemitem>.</para>
    <programlisting><?db-font-size 75% ?>$ xlC -qarch=pwr8 -qaltivec test.cxx -o test.exe</programlisting>
    <para>XLC consumes <systemitem>-g</systemitem> and <systemitem>-O3</systemitem>. If you want Position Independent Code then use <systemitem>-qpic</systemitem> for XLC.</para>
    <para>XLC uses integer math to calculate the effective memory addresses from a memory address and offset. That is, if you have an array <systemitem>uint32_t vals = {0,1,2,3,4,5,6,7}</systemitem>, and you call <systemitem>uint32x4_p v = vec_xl(vals, 4)</systemitem>, then the vector will have the value <systemitem>{1,2,3,4}</systemitem>. That is, XLC uses the address of <systemitem>vals</systemitem> and simply adds <systemitem>4</systemitem> to it. Effective addresses are discussed in <xref linkend="effective_address"/>, and loads and stores are discussed in <xref linkend="altivec_aligned_ld_st"/>.</para>
    <para>IBM XLC switched to a LLVM front-end for the Linux compiler at version 13.1 (AIX still uses the IBM front-end). When compiling with XLC version 13.1 or higher on Linux you may need the <systemitem>-qcompatmacros</systemitem> compiler option. Without <systemitem>-qcompatmacros</systemitem> the samples fail to compile because XLC 13.1 does not define <systemitem>__xlc__</systemitem> or <systemitem>__xlC__</systemitem>. However, LLVM defines <systemitem>__GNUC__</systemitem> but compilation fails because the compiler cannot consume the GCC POWER8 builtins.<footnote><para>LLVM behavior is befuddling to some people. It is the XLC compiler but fails to define <systemitem>__xlc__</systemitem> or <systemitem>__xlC__</systemitem>. Then it defines <systemitem>__GNUC__</systemitem> but fails to consume the GCC POWER8 builtins.</para></footnote></para>
  </section>
  <section id="clang_compiler" xreflabel="Clang Compiler">
    <title>Clang compiler</title>
    <para><indexterm><primary>LLVM</primary></indexterm><indexterm><primary>Clang</primary></indexterm>Compiling a test program with LLVM's Clang will generally look like below. The important part is <systemitem>-mcpu=power8</systemitem> which selects the POWER8 Instruction Set Architecture (ISA). The minimum architecture required for Altivec with Clang is unknown. The Clang compiler was not tested because the <link linkend="intro_cfarm">compile farm </link> lacks a Clang installation.</para>
    <programlisting><?db-font-size 75% ?>$ g++ -mcpu=power8 -maltivec test.cxx -o test.exe</programlisting>
    <para>Clang consumes <systemitem>-g</systemitem> and <systemitem>-O3</systemitem>. If you want Position Independent Code then use <systemitem>-fPIC</systemitem>.</para>
    <para><indexterm><primary>-qxlcompatmacros</primary></indexterm><indexterm><primary>-qshowmacros</primary></indexterm>IBM switched to an LLVM front-end for the Linux compiler at version 13.1 as discussed in <xref linkend="xlc_compiler"/>. Be mindful of how you use preprocessor macros when LLVM is the front-end because the compiler claims to be three different compilers â€” GCC, Clang and XLC.</para>
    <programlisting><?db-font-size 75% ?>$ xlc -qxlcompatmacros -qshowmacros -E test.cxx | \
  egrep -i -E 'xlc|clang|llvm|gnuc'
#define __GNUC_GNU_INLINE__ 1
#define __GNUC_MINOR__ 2
#define __GNUC_PATCHLEVEL__ 1
#define __GNUC__ 4
#define __clang__ 1
#define __clang_major__ 4
#define __clang_minor__ 0
#define __clang_patchlevel__ 1
#define __llvm__ 1
#define __xlC__ 0x0d01
#define __xlC_ver__ 0x00000601</programlisting>
    <para>The front-end change triggered <ulink url="http://bugs.llvm.org/show_bug.cgi?id=38378">LLVM Issue 38378</ulink> for one project because the compiler entered GCC code paths but it cannot consume GCC POWER8 builtins.</para>
    <para>Clang uses pointer math to calculate the effective memory addresses from a memory address and offset. That is, if you have an array <systemitem>uint32_t vals = {0,1,2,3,4,5,6,7}</systemitem>, and you call <systemitem>uint32x4_p v = vec_xl(vals, 4)</systemitem>, then the vector will have the value <systemitem>{4,5,6,7}</systemitem>. That is, Clang uses the address of <systemitem>vals</systemitem> and adds <systemitem>4*sizeof(uint32_t)</systemitem> to it. Effective addresses are discussed in <xref linkend="effective_address"/>, and loads and stores are discussed in <xref linkend="altivec_aligned_ld_st"/>.</para>
    <para>If you use the Clang compiler then be sure it is version 7.1 or above due to <ulink url="https://bugs.llvm.org/show_bug.cgi?id=39704">Bug 39704</ulink>. The 39704 bug is due to Clang discarding code it believed was illegal due to an unaligned Altivec load. The code was legal because POWER7 and VSX provides the load of data using natural alignment. The code should not have been removed.</para>
  </section>
  <section id="altivec_header" xreflabel="Altivec Headers">
    <title>Altivec headers</title>
    <para>The header file required for datatypes and functions is <systemitem>&lt;altivec.h&gt;</systemitem>. You must enable Altivec using compiler options <systemitem>-maltivec</systemitem> or <systemitem>-qaltivec</systemitem> as discussed in <xref linkend="gcc_compiler"/> and <xref linkend="xlc_compiler"/>.</para>
    <para>To support C++ projects and compilers the <systemitem>__vector</systemitem> keyword is used rather than <systemitem>vector</systemitem>. A typical Altivec include looks as shown below.</para>
    <programlisting><?db-font-size 75% ?>#if defined(__ALTIVEC__)
# include &lt;altivec.h&gt;
# undef vector
# undef pixel
# undef bool
#endif</programlisting>
    <para>The <systemitem>__ALTIVEC__</systemitem> preprocessor macro will be defined when using <systemitem>-qaltivec</systemitem> or <systemitem>-maltivec</systemitem> compiler options as discussed in <xref linkend="gcc_compiler"/> and <xref linkend="xlc_compiler"/>. Macros and definitions are discussed more in <xref linkend="preprocessor_macros"/>.</para>
    <para>When you need to use the vector datatypes you use the <systemitem>__vector</systemitem> keyword as discussed in the section <xref linkend="power8_datatypes"/>.</para>
    <programlisting><?db-font-size 75% ?>/* A vector with four elements initialized to 0 */
__vector unsigned int v = {0,0,0,0};</programlisting>
  </section>
  <section id="preprocessor_macros" xreflabel="Preprocessor Macros">
    <title>Preprocessor macros</title>
    <para><indexterm><primary>preprocessor</primary></indexterm><indexterm><primary>macros</primary></indexterm>The following preprocessor macros and defines will be encountered depending on the platform and compiler:</para>
    <itemizedlist mark="bullet">
      <listitem>
        <para><systemitem>__powerpc__</systemitem> and <systemitem>__powerpc</systemitem> on AIX
        </para>
      </listitem>
      <listitem>
        <para><systemitem>__powerpc__</systemitem> and <systemitem>__powerpc64__</systemitem> on Linux
        </para>
      </listitem>
      <listitem>
        <para><systemitem>_ARCH_PWR3</systemitem> through <systemitem>_ARCH_PWR9</systemitem> on AIX and Linux
        </para>
      </listitem>
      <listitem>
        <para><systemitem>__linux__</systemitem>, <systemitem>__linux</systemitem> and <systemitem>linux</systemitem> on Linux
        </para>
      </listitem>
      <listitem>
        <para><systemitem>_AIX</systemitem>, and <systemitem>_AIX32</systemitem> through <systemitem>_AIX72</systemitem> on AIX
        </para>
      </listitem>
      <listitem>
        <para><systemitem>__GNUC__</systemitem> when using GCC C/C++ compiler
        </para>
      </listitem>
      <listitem>
        <para><systemitem>__xlc__</systemitem> and <systemitem>__xlC__</systemitem> when using IBM XL C/C++
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section id="power8_endianness" xreflabel="Machine Endianness">
    <title>Machine endianness</title>
    <para><indexterm><primary>__BYTE_ORDER__</primary></indexterm><indexterm><primary>__ORDER_LITTLE_ENDIAN__</primary></indexterm><indexterm><primary>__ORDER_BIG_ENDIAN__</primary></indexterm><indexterm><primary>__LITTLE_ENDIAN__</primary></indexterm><indexterm><primary>__BIG_ENDIAN__</primary></indexterm>You will experience both little-endian and big-endian machines in the field when working with a modern PowerPC architecture. Linux is generally little-endian, while AIX is big-endian.</para>
    <para>When writing portable source code you should check the value of preprocessor macros <systemitem>__LITTLE_ENDIAN__</systemitem> or <systemitem>__BIG_ENDIAN__</systemitem> to determine the configuration. The value of the macros <systemitem>__BIG_ENDIAN__</systemitem> and <systemitem>__LITTLE_ENDIAN__</systemitem> are defined to non-0 when in effect. Source code checking endianness should look similar to the code shown below.</para>
    <programlisting><?db-font-size 75% ?>#if __LITTLE_ENDIAN__
# pragma message "Little-endian system"
#elif __BIG_ENDIAN__
# pragma message "Big-endian system"
#else
# error WTF???
#endif</programlisting>
    <para>The compilers can show the endian-related preprocessor macros available on a platform. Below is from GCC on <systemitem>gcc112</systemitem> from the <link linkend="intro_cfarm">compile farm</link>, which is ppc64-le.</para>
    <screen>$ g++ -dM -E test.cxx | grep -i endian
#define __ORDER_LITTLE_ENDIAN__ 1234
#define _LITTLE_ENDIAN 1
#define __FLOAT_WORD_ORDER__ __ORDER_LITTLE_ENDIAN__
#define __ORDER_PDP_ENDIAN__ 3412
#define __LITTLE_ENDIAN__ 1
#define __ORDER_BIG_ENDIAN__ 4321
#define __BYTE_ORDER__ __ORDER_LITTLE_ENDIAN__
</screen>
    <para><indexterm><primary>-qshowmacros</primary></indexterm>And the complimentary view from IBM XL C/C++ on <systemitem>gcc112</systemitem> from the <xref linkend="intro_cfarm"/>, which is ppc64-le.</para>
    <screen>$ xlC -qshowmacros -E test.cxx | grep -i endian
#define _LITTLE_ENDIAN 1
#define __BYTE_ORDER__ __ORDER_LITTLE_ENDIAN__
#define __FLOAT_WORD_ORDER__ __ORDER_LITTLE_ENDIAN__
#define __LITTLE_ENDIAN__ 1
#define __ORDER_BIG_ENDIAN__ 4321
#define __ORDER_LITTLE_ENDIAN__ 1234
#define __ORDER_PDP_ENDIAN__ 3412
#define __VEC_ELEMENT_REG_ORDER__ __ORDER_LITTLE_ENDIAN__
</screen>
    <para>However, below is <systemitem>gcc119</systemitem> from the <link linkend="intro_cfarm">compile farm</link>, which is ppc64-be. It runs AIX and notice <systemitem>__BYTE_ORDER__</systemitem>, <systemitem>__ORDER_BIG_ENDIAN__</systemitem> and <systemitem>__ORDER_LITTLE_ENDIAN__</systemitem> are not present.</para>
    <screen>$ xlC -qshowmacros -E test.cxx | grep -i endian
#define __BIG_ENDIAN__ 1
#define _BIG_ENDIAN 1
#define __THW_BIG_ENDIAN__ 1
#define __HHW_BIG_ENDIAN__ 1
</screen>
    <para><indexterm><primary>SPARCv8</primary></indexterm>PowerPC is not the only architecture to have the <systemitem>__BIG_ENDIAN__</systemitem> and <systemitem>__LITTLE_ENDIAN__</systemitem> trap. The trap is also laid on SPARCv8, and caused BIND users on NetBSD-8 to fail to validate DNSSEC keys. The problem was the endianness tests used <systemitem>__ORDER_BIG_ENDIAN__</systemitem> and <systemitem>__ORDER_LITTLE_ENDIAN__</systemitem>. Also see <ulink url="https://mail-index.netbsd.org/netbsd-users/2020/04/16/msg024529.html">DNSSEC vs netbsd-8/sparc?</ulink>.</para>
    <para>If you wish to use the GCC macros for endianness test, then you should craft your test like below. The tests below will perform properly with Clang, GCC and XLC.</para>
    <programlisting><?db-font-size 75% ?>
#if (defined(__BYTE_ORDER__ ) &amp;&amp; \
    (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)) || \
     defined(__LITTLE_ENDIAN__)
# define MYLIB_LITTLE_ENDIAN 1
#endif

#if (defined(__BYTE_ORDER__ ) &amp;&amp; \
    (__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)) || \
     defined(__BIG_ENDIAN__)
# define MYLIB_BIG_ENDIAN 1
#endif

#if defined(MYLIB_LITTLE_ENDIAN) &amp;&amp; defined(MYLIB_BIG_ENDIAN)
# error WTF???
#endif</programlisting>
  </section>
  <section id="power8_alloc" xreflabel="Memory Allocation">
    <title>Memory allocation</title>
    <para><indexterm><primary>malloc</primary></indexterm><indexterm><primary>calloc</primary></indexterm>System calls like <systemitem>malloc</systemitem> and <systemitem>calloc</systemitem> (and friends) are used to acquire memory from the heap. The system calls <emphasis>do not</emphasis> guarantee alignment to any particular boundary on all platforms. Linux generally returns a pointer that is at least 16-byte aligned on all platforms, including ARM, PPC, MIPS and x86. AIX <emphasis>does not</emphasis> provide the same <ulink url="http://stackoverflow.com/q/48373188/608639">alignment behavior</ulink>.</para>
    <para><indexterm><primary>posix_memalign</primary></indexterm>To avoid unexpected surprises when using heap allocations you should use <ulink url="http://pubs.opengroup.org/onlinepubs/009695399/functions/posix_memalign.html"><systemitem>posix_memalign</systemitem></ulink> to acquire heap memory aligned to a particular boundary and <systemitem>free</systemitem> to return it to the system.</para>
    <para><indexterm><primary>AIX</primary></indexterm><indexterm><primary>vec_malloc</primary></indexterm><indexterm><primary>vec_free</primary></indexterm>AIX provides routines for vector memory allocation and alignment. They are <systemitem>vec_malloc</systemitem> and <systemitem>vec_free</systemitem>, and you can use them like <systemitem>_mm_malloc</systemitem> on Intel machines with Streaming SIMD Extensions (SSE).</para>
  </section>
  <section id="power8_datatypes" xreflabel="Vector Datatypes">
    <title>Vector datatypes</title>
    <para><indexterm><primary>__vector</primary></indexterm>Three vector datatypes are needed for vector programming. The three types used for crypto are listed below.</para>
    <itemizedlist mark="bullet">
      <listitem>
        <para>
          <systemitem>__vector unsigned char</systemitem>
        </para>
      </listitem>
      <listitem>
        <para>
          <systemitem>__vector unsigned int</systemitem>
        </para>
      </listitem>
      <listitem>
        <para>
          <systemitem>__vector unsigned long long</systemitem>
        </para>
      </listitem>
    </itemizedlist>
    <para><indexterm><primary>uint8x16_p</primary></indexterm><indexterm><primary>uint32x4_p</primary></indexterm><systemitem>__vector unsigned char</systemitem> is arranged as 16 each 8-bit bytes, and it is a typedef to <systemitem>uint8x16_p</systemitem>. <systemitem>__vector unsigned int</systemitem> is arranged as 4 each 32-bit words, and it is a typedef to <systemitem>uint32x4_p</systemitem>.</para>
    <para><indexterm><primary>uint64x2_p</primary></indexterm>POWER7 and VSX added <systemitem>__vector unsigned long long</systemitem>, and it is arranged as 2 each 64-bit double words. The typedef is <systemitem>uint64x2_p</systemitem>. According to the Power Architecture ELF ABI Specification, you should always use <systemitem>__vector unsigned long long</systemitem> for portability because it is available on 32-bit and 64-bit systems. You should not use <systemitem>__vector unsigned long</systemitem>.<footnote><para>On 32-bit systems a <systemitem>long</systemitem> is 32-bits under the ILP32 data model. A <systemitem>__vector unsigned long</systemitem> on a 32-bit system would be equivalent to a <systemitem>__vector unsigned int</systemitem> with 4 elements.</para></footnote></para>
    <para>Though POWER7 and VSX added <systemitem>__vector unsigned long long</systemitem>, the associated  64-bit vector operations did not arrive until POWER8. That means you can load a <systemitem>__vector unsigned long long</systemitem>, but you can't add, subtract, or, or xor them until POWER8. You will be OK with crypto operations since crypto is POWER8.</para>
    <para>The typedef naming was selected to convey the arrangement, like <systemitem>32x4</systemitem> and <systemitem>64x2</systemitem>. The trailing <systemitem>_p</systemitem> was selected to convey the POWER architecture and avoid collisions with ARM NEON vector data types. The suffix <systemitem>_p8</systemitem> (for POWER8 architecture) or <systemitem>_v</systemitem> (for Vector) would work just as well. You should avoid <systemitem>_t</systemitem> because it is reserved for the language by the C standards.</para>
  </section>
  <section id="effective_address" xreflabel="Effective Addresses">
    <title>Effective addresses</title>
    <para>Some vector operations, like loads and stores, can be sensitive to the alignment of a memory address. Operations like <systemitem>vec_ld</systemitem> and <systemitem>vec_st</systemitem> are sensitive, and the documentation clearly states it.</para>
    <para>The effective address is a simple sum consisting of the memory address plus the offset into the address with the result rounded down to a multiple of 16. Effective addresses follow integer arithmetic and not pointer arithmetic.</para>
    <para>You can calculate an effective address using the following code. Notice the bottom 4 bits are masked after calculating the sum to yield a multiple of 16.</para>
    <programlisting><?db-font-size 75% ?>uintptr_t maddr = (uintptr_t)mem_addr;
uintptr_t mask = ~(uintptr_t)0xf;
uintptr_t eaddr = (maddr+offset) &amp; mask;</programlisting>
    <para><systemitem>vec_ld</systemitem> takes a pointer and an offset to load a value into a VSX register. Each of the following yield the same VSX register value because the effective addresses are the same. (Old x86 programmers should reminisce on  segmented memory).</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>vec_ld</primary></indexterm>uint8_t* ptr1 = 0x401000;
int off1 = 32;
uint8x16_p r1 = vec_ld(off1, ptr1);

uint8_t* ptr2 = 0x401010;
int off2 = 16;
uint8x16_p r2 = vec_ld(off2, ptr2);</programlisting>
    <para>The following also yields the same VSX register value because the effective address is the same. If you truly wanted to load 4 bytes beyond <systemitem>ptr</systemitem> then you loaded the wrong value because <systemitem>(0x401010+4)&amp;0xfffffff0 = 0x401010</systemitem>.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>vec_ld</primary></indexterm>uint8_t* ptr1 = 0x401000;
int off1 = 16;
uint8x16_p r1 = vec_ld(off1, ptr1);

uint8_t* ptr2 = 0x401010;
int off2 = 4;
uint8x16_p r2 = vec_ld(off2, ptr2);</programlisting>
    <para>The application of effective addresses are discussed more below in <xref linkend="altivec_aligned_ld_st"/> and <xref linkend="vsx_unaligned_ld_st"/>.</para>
  </section>
  <section id="altivec_aligned_ld_st" xreflabel="Aligned Data References">
    <title>Aligned data references</title>
    <para>Altivec loads and stores have traditionally been performed using <systemitem>vec_ld</systemitem> and <systemitem>vec_st</systemitem> since the early days of Altivec. <systemitem>vec_ld</systemitem> and <systemitem>vec_st</systemitem> are sensitive to alignment of the effective address.</para>
    <para>Altivec <emphasis>does not</emphasis> raise a <indexterm><primary>SIGBUS</primary></indexterm><systemitem>SIGBUS</systemitem> to indicate a misaligned load or store. Instead, the bottom 4 bits of the sum <systemitem>address+offset</systemitem> are masked-off and then the memory at the effective address is loaded.</para>
    <para>You can use the Altivec loads and stores when you <emphasis>control</emphasis> buffers and ensure they are 16-byte aligned, like an AES key schedule table. Otherwise just use <link linkend="vsx_unaligned_ld_st">unaligned loads and stores</link> to avoid trouble.</para>
    <para>The C/C++ code to perform a load using <indexterm><primary>vec_ld</primary></indexterm><systemitem>vec_ld</systemitem> should look similar to below. Notice the <systemitem>assert</systemitem> to warn you of problems in debug builds.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecLoad</primary></indexterm><indexterm><primary>vec_ld</primary></indexterm>template &lt;class T&gt;
uint32x4_p VecLoad(const T* mem_addr, int offset)
{
#ifndef NDEBUG
    uintptr_t maddr = (uintptr_t)mem_addr;
    uintptr_t mask = ~(uintptr_t)0xf;
    uintptr_t eaddr = (maddr+offset) &amp; mask;
    assert(maddr == eaddr);
#endif

    return (uint32x4_p)vec_ld(offset, mem_addr);
}</programlisting>
    <para>The C/C++ code to perform a store using <indexterm><primary>vec_st</primary></indexterm><systemitem>vec_st</systemitem> should look similar to below.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecStore</primary></indexterm><indexterm><primary>vec_st</primary></indexterm>template &lt;class T&gt;
void VecStore(const uint32x4_p val, T* mem_addr, int offset)
{
#ifndef NDEBUG
    uintptr_t maddr = (uintptr_t)mem_addr;
    uintptr_t mask = ~(uintptr_t)0xf;
    uintptr_t eaddr = (maddr+offset) &amp; mask;
    assert(maddr == eaddr);
#endif

    vec_st((uint8x16_p)val, offset, mem_addr);
}</programlisting>
  </section>
  <section id="vsx_unaligned_ld_st" xreflabel="Unaligned Data References">
    <title>Unaligned data references</title>
    <para>POWER7 and VSX introduced loads and stores for 32-bit and 64-bit datatypes that avoid the 16-byte aligned memory requirements. POWER9 introduced loads and stores for 8-bit and 16-bit datatypes. In this context "unaligned" means alignment less than 16-bytes required by Altivec, but alignment still must be natural. While POWER9 allows you to load a byte array, POWER7 and POWER8 limits you to 32-bit and 64-bit arrays. If you have a byte array on POWER7, then it must be at least 32-bit or 64-bit aligned or you have to use the Altivec load.</para>
    <para>The preferred builtin functions for 32-bit and 64-bit datatype loads and stores are <indexterm><primary>vec_xl</primary></indexterm><systemitem>vec_xl</systemitem> and <indexterm><primary>vec_xst</primary></indexterm><systemitem>vec_xst</systemitem>. The builtins are available on all currently supported versions of GCC and XLC. However, older versions of GCC such as those installed on many enterprise Linux distributions do not supply them. For compatibility with these older compilers you may use <indexterm><primary>vec_vsx_ld</primary></indexterm><systemitem>vec_vsx_ld</systemitem> and <indexterm><primary>vec_vsx_st</primary></indexterm><systemitem>vec_vsx_st</systemitem> for GCC.</para>
    <para>You should use the POWER7 and VSX loads and stores whenever you <emphasis>do not control</emphasis> buffers or their alignments, like a message in a buffer supplied by the user.</para>
    <para>The C/C++ code to perform a load using <systemitem>vec_xl</systemitem> and <systemitem>vec_vsx_ld</systemitem> should look similar to below. The function name has a <systemitem>u</systemitem> added to indicate unaligned.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecLoadu</primary></indexterm><indexterm><primary>vec_xl</primary></indexterm><indexterm><primary>vec_vsx_ld</primary></indexterm>template &lt;class T&gt;
uint32x4_p VecLoadu(const T* mem_addr, int offset)
{
#if defined(__xlc__) || defined(__xlC__)
    return (uint32x4_p)vec_xl(offset, mem_addr);
#else
    return (uint32x4_p)vec_vsx_ld(offset, mem_addr);
#endif
}</programlisting>
    <para>The C/C++ code to perform a store using <systemitem>vec_xst</systemitem> and <systemitem>vec_vsx_st</systemitem> should look similar to below.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecStoreu</primary></indexterm><indexterm><primary>vec_xst</primary></indexterm><indexterm><primary>vec_vsx_st</primary></indexterm>template &lt;class T&gt;
void VecStoreu(const uint32x4_p val, T* mem_addr, int offset)
{
#if defined(__xlc__) || defined(__xlC__)
    vec_xst((uint8x16_p)val, offset, mem_addr);
#else
    vec_vsx_st((uint8x16_p)val, offset, mem_addr);
#endif
}</programlisting>
    <para>If your code will only be compiled with supported compilers, you may simplify it to use the <systemitem>vec_xl</systemitem> and <systemitem>vec_xst</systemitem> variants for both XLC and GCC.</para>
  </section>
  <section id="power8_vec_deref" xreflabel="Vector Dereferences">
    <title>Vector dereferences</title>
    <para>
      The <ulink url="https://openpowerfoundation.org/?resource_lib=64-bit-elf-v2-abi-specification-power-architecture">OpenPOWER ELF V2 ABI Specification</ulink>, version 1.4, incorrectly states that accessing vectors on Power should preferably be done with vector pointers and the dereference <systemitem>operator*</systemitem>. However, this is only permitted for aligned vector references.  Examples in Chapter 6 of the ABI document show use of casting operations that represent undefined behavior according to the C standard.  An errata document that corrects the ABI may be found <ulink url="https://openpowerfoundation.org/?resource_lib=openpower-elfv2-errata-elfv2-abi-version-1-4">at the OpenPOWER Foundation website</ulink>. Subsequent sections describe the proper way to use loads and stores of aligned and unaligned data.
    </para>
  </section>
  <section id="power8_shift" xreflabel="Vector Shifts">
    <title>Vector shifts</title>
    <para>Altivec shifts and rotates are performed using <emphasis>Vector Shift Left Double by Octet Immediate</emphasis>. The vector shift and rotate builtin is <indexterm><primary>vec_sld</primary></indexterm><systemitem>vec_sld</systemitem> and it compiles/assembles to <indexterm><primary>vsldoi</primary></indexterm><systemitem>vsldoi</systemitem>. Both shift and rotate operate on a concatenation of two vectors. Bytes are shifted out on the left and shifted in on the right. The instructions need an integral constant in the range 0 - 15, inclusive.</para>
    <para>Vector shifts and rotates perform as expected on big-endian machines. Little-endian machines require special handling to produce correct results. The catch is, <ulink url="http://www.ibm.com/support/knowledgecenter/SSXVZZ_13.1.4/com.ibm.xlcpp1314.lelinux.doc/compiler_ref/vec_sld.html">IBM manuals don't tell you about it</ulink>.</para>
    <para>The issue is <ulink url="http://stackoverflow.com/q/46341923/608639">shifts and rotates are endian sensitive</ulink> and you have to use <systemitem>16-n</systemitem> integrals and swap vector arguments on little-endian systems. The C++ source code provides the following template function to compensate for the little-endian behavior.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecShiftLeft</primary></indexterm><indexterm><primary>vec_sld</primary></indexterm>template &lt;unsigned int N, class T&gt;
T VecShiftLeft(const T val1, const T val2)
{
#if __LITTLE_ENDIAN__
    enum {R = (16-N)&amp;0xf};
    return vec_sld(val2, val1, R);
#else
    enum {R = N&amp;0xf};
    return vec_sld(val1, val2, R);
#endif
}</programlisting>
    <para>A <indexterm><primary>VecRotateLeft</primary></indexterm><systemitem>VecRotateLeft</systemitem> would be similar to the code below, if needed. Rotate is a special case of shift where both vector arguments are the same value.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>VecRotateLeft</primary></indexterm><indexterm><primary>vec_sld</primary></indexterm>template &lt;unsigned int N, class T&gt;
T VecRotateLeft(const T val)
{
#if __LITTLE_ENDIAN__
    enum {R = (16-N)&amp;0xf};
    return vec_sld(val, val, R);
#else
    enum {R = N&amp;0xf};
    return vec_sld(val, val, R);
#endif
}</programlisting>
  </section>
  <section id="power8_permute" xreflabel="Vector Permutes">
    <title>Vector permutes</title>
    <para>Vector permutes allow you to rearrange elements in a vector. The values to be permuted can be in any arrangement like 64x2 or 32x4, but the mask is always an octet mask using an 8x16 arrangement.</para>
    <para>The Altivec permute is very powerful and it stands out among architectures like ARM, Aarch64 and x86. The instruction allows you to select elements from two source vectors. When an index in the mask is in the range <systemitem>[0,15]</systemitem> then elements from the first vector are selected, and index values in the the range <systemitem>[16,31]</systemitem> select elements from the second vector.</para>
    <para>As an example, suppose you have a big-endian byte array like a message to be hashed using SHA-256. SHA operates on 32-bit words so the message needs a shuffle on little-endian systems. The code to perform the permute on a little-endian machine would look like below.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>vec_perm</primary></indexterm>uint32x4_p msg = vec_ld(/*load from memory*/);
uint8x16_p mask = {3,2,1,0, 7,6,5,4, 11,10,9,8, 15,14,13,12};
msg = vec_perm(msg, msg, mask);</programlisting>
    <para>The previous example only needed one vector so it used <systemitem>msg</systemitem> twice in the call to <systemitem>vec_perm</systemitem>. The Altivec code is similar to <systemitem>_mm_shuffle_epi8</systemitem> on Intel machines. An example that interleaves two different vectors is shown below.</para>
    <programlisting><?db-font-size 75% ?><indexterm><primary>vec_perm</primary></indexterm>uint32x4_p a = { 0,  0,  0,  0};  // All 0 bits
uint32x4_p b = {-1, -1, -1, -1};  // All 1 bits
uint8x16_p m = {0,1,2,3, 16,17,18,19, 4,5,6,7, 20,21,22,23};
uint32x4_p c = vec_perm(a, b, m);</programlisting>
    <para>After the code above executes the vector <systemitem>c</systemitem> will have the value <systemitem>{0, -1, 0, -1}</systemitem>.</para>
    <para>Below is the image IBM provides for the <systemitem>vec_perm</systemitem> documentation. The IBM example shows <systemitem>d = vec_perm(a, b, c)</systemitem>. The light gray blocks in vector <systemitem>d</systemitem> are from the first vector, and dark gray blocks in vector <systemitem>d</systemitem> are from the second vector.</para>
    <para/>
    <mediaobject>
      <imageobject>
        <imagedata fileref="images/vecperm.gif" width="5.25in" scalefit="1" format="GIF"/>
      </imageobject>
      <textobject>
        <phrase>Permute 16 8-bit integer elements.</phrase>
      </textobject>
    </mediaobject>
  </section>
  <section id="altivec_32" xreflabel="32-bit Altivec">
    <title>32-bit Altivec</title>
    <para>Most PowerPCs, from the early days through POWER7, provide an Altivec unit that is only capable of packed 32-bit operations. POWER7 and VSX added <systemitem>__vector unsigned long long</systemitem>, and POWER8 added 64-bit operations like add and subtract, but none of them are available on older Altivec. Some algorithms you implement may need 64-bit datatypes on the older units. The algorithms include <indexterm><primary>Salsa</primary></indexterm>Salsa, <indexterm><primary>ChaCha</primary></indexterm>ChaCha, <indexterm><primary>Simon-128</primary></indexterm>Simon-128 and <indexterm><primary>Speck-128</primary></indexterm>Speck-128. In this case you can implement your own <indexterm><primary>VecAdd64</primary></indexterm><systemitem>VecAdd64</systemitem> and <indexterm><primary>VecSub64</primary></indexterm><systemitem>VecSub64</systemitem> using 32-bit datatypes.</para>
    <para>The code below provides 64-bit add using 32-bit vectors.</para>
    <programlisting><?db-font-size 75% ?>inline uint32x4_p
VecAdd64(const uint32x4_p vec1, const uint32x4_p vec2)
{
    // 64-bit elements available at POWER7 with VSX,
    // but addudm requires POWER8
#if defined(_ARCH_PWR8)
    return (uint32x4_p)vec_add((uint64x2_p)vec1, (uint64x2_p)vec2);
#else
    // The carry mask selects carrys for elements 1 and 3 and sets
    // remaining elements to 0. The results is then shifted so the
    // carried values are added to elements 0 and 2.
#if defined(__BIG_ENDIAN__)
    const uint32x4_p zero = {0, 0, 0, 0};
    const uint32x4_p mask = {0, 1, 0, 1};
#else
    const uint32x4_p zero = {0, 0, 0, 0};
    const uint32x4_p mask = {1, 0, 1, 0};
#endif

    uint32x4_p cy = vec_addc(vec1, vec2);
    uint32x4_p res = vec_add(vec1, vec2);
    cy = vec_and(mask, cy);
    cy = vec_sld(cy, zero, 4);
    return vec_add(res, cy);
#endif
}</programlisting>
    <para>The code below provides 64-bit subtract using 32-bit vectors. The code is slightly more complex than <systemitem>VecAdd64</systemitem> because the borrow is generated as a compliment.</para>
    <programlisting><?db-font-size 75% ?>inline uint32x4_p
VecSub64(const uint32x4_p vec1, const uint32x4_p vec2)
{
#if defined(_ARCH_PWR8)
    // 64-bit elements available at POWER7 with VSX,
    // but subudm requires POWER8
    return (uint32x4_p)vec_sub((uint64x2_p)vec1, (uint64x2_p)vec2);
#else
    // The borrow mask selects borrows for elements 1 and 3 and sets
    // remaining elements to 0. The results is then shifted so the
    // borrowed values are subtracted from elements 0 and 2.
#if defined(__BIG_ENDIAN__)
    const uint32x4_p zero = {0, 0, 0, 0};
    const uint32x4_p mask = {0, 1, 0, 1};
#else
    const uint32x4_p zero = {0, 0, 0, 0};
    const uint32x4_p mask = {1, 0, 1, 0};
#endif

    uint32x4_p bw = vec_subc(vec1, vec2);
    uint32x4_p res = vec_sub(vec1, vec2);
    bw = vec_andc(mask, bw);
    bw = vec_sld(bw, zero, 4);
    return vec_sub(res, bw);
#endif
}</programlisting>
    <para>While <systemitem>VecAdd64</systemitem> and <systemitem>VecSub64</systemitem> look inefficient they are profitable for some algorithms. For example, on a PowerMac G5, Simon-128 and Speck-128 run about 6x faster in the 32-bit Altivec unit compared to a C/C++ implementation in the integer unit.</para>
  </section>
</chapter>
